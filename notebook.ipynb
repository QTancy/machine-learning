{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5723aa0a-cd62-4a66-8dea-e2468408d585",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Easy OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from app.services.qcap import process_qcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcfb160b-2109-4203-8757-234a2e767dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n",
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "reader = easyocr.Reader(['id'], gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc6b7e6-7a66-4dfa-9357-7ffcfb507f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghiffari\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([[57, 483], [71, 483], [71, 509], [57, 509]], '1', 0.9986463600755577),\n",
       " ([[115, 475], [461, 475], [461, 517], [115, 517]],\n",
       "  'SPGTHY BOLOGNASE',\n",
       "  0.3772504946452746),\n",
       " ([[513, 477], [637, 477], [637, 521], [513, 521]],\n",
       "  '58,OQo',\n",
       "  0.5079821943176018),\n",
       " ([[55, 529], [71, 529], [71, 557], [55, 557]], '1', 0.9997164212072427),\n",
       " ([[113, 520], [256, 520], [256, 562], [113, 562]],\n",
       "  'PEPPER',\n",
       "  0.9985323994993548),\n",
       " ([[267, 521], [341, 521], [341, 561], [267, 561]],\n",
       "  'AVs',\n",
       "  0.25621664945060624),\n",
       " ([[494, 515], [641, 515], [641, 566], [494, 566]],\n",
       "  '165,Ooo',\n",
       "  0.4151908293778719),\n",
       " ([[137, 569], [233, 569], [233, 609], [137, 609]],\n",
       "  'WELL',\n",
       "  0.9982210993766785),\n",
       " ([[247, 565], [343, 565], [343, 607], [247, 607]],\n",
       "  'DONE',\n",
       "  0.6025109968494945),\n",
       " ([[112, 610], [384, 610], [384, 658], [112, 658]],\n",
       "  'WAGYU RIBEYE',\n",
       "  0.8970899244688244),\n",
       " ([[498, 606], [646, 606], [646, 654], [498, 654]],\n",
       "  '195,Ooo',\n",
       "  0.7764594667594529),\n",
       " ([[136, 656], [386, 656], [386, 706], [136, 706]],\n",
       "  'MEDIUM WELL',\n",
       "  0.6529069054318408),\n",
       " ([[53, 721], [67, 721], [67, 753], [53, 753]], '1', 0.9998729269344864),\n",
       " ([[112, 702], [430, 702], [430, 756], [112, 756]],\n",
       "  'ICED LEMON TEA',\n",
       "  0.804111533836303),\n",
       " ([[520, 698], [650, 698], [650, 746], [520, 746]],\n",
       "  '22,Ooo',\n",
       "  0.3555958589123042),\n",
       " ([[53, 771], [67, 771], [67, 803], [53, 803]], '1', 0.9917820947108034),\n",
       " ([[109, 747], [473, 747], [473, 808], [109, 808]],\n",
       "  'FUSION TEA LYCHE',\n",
       "  0.48262847347107646),\n",
       " ([[524, 742], [652, 742], [652, 792], [524, 792]],\n",
       "  '28 , OQo',\n",
       "  0.479597721732074),\n",
       " ([[53, 823], [67, 823], [67, 855], [53, 855]], '1', 0.9989809011380579),\n",
       " ([[107, 793], [476, 793], [476, 859], [107, 859]],\n",
       "  'NUTTELA BRDWNIES',\n",
       "  0.6472932805087331),\n",
       " ([[523, 793], [577, 793], [577, 835], [523, 835]], '35', 0.9996467433936705),\n",
       " ([[583, 789], [653, 789], [653, 831], [583, 831]],\n",
       "  'Ooo',\n",
       "  0.19378657421860776),\n",
       " ([[508, 880], [659, 880], [659, 934], [508, 934]],\n",
       "  '503,OQ7',\n",
       "  0.12604696161837955),\n",
       " ([[109, 902], [308, 902], [308, 964], [109, 964]],\n",
       "  'SUBtoTAL',\n",
       "  0.15580257724771862),\n",
       " ([[527, 927], [663, 927], [663, 988], [527, 988]],\n",
       "  '52,815',\n",
       "  0.9994952946994511),\n",
       " ([[113, 965], [187, 965], [187, 1009], [113, 1009]],\n",
       "  'PB1',\n",
       "  0.5902896735490853),\n",
       " ([[532, 974], [664, 974], [664, 1030], [532, 1030]],\n",
       "  '25,150',\n",
       "  0.9904275389072467),\n",
       " ([[111, 1003], [313, 1003], [313, 1065], [111, 1065]],\n",
       "  'SVC ChrG',\n",
       "  0.3883312467559501),\n",
       " ([[104, 1055], [270, 1055], [270, 1154], [104, 1154]],\n",
       "  'DUE',\n",
       "  0.9684556294131638),\n",
       " ([[333, 1019], [651, 1019], [651, 1134], [333, 1134]],\n",
       "  '580,965',\n",
       "  0.8256331780719026)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = reader.readtext(\"./inference/jpgs/img_00001.jpg\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46eeb49e-59f1-44fb-a1d5-8f34e8ff682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lines = [res[1] for res in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd11d8a6-11da-4570-a768-dc5c74c85e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined  = \" \".join(text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2241f654-0f01-4224-9c22-18d51ab50569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 SPGTHY BOLOGNASE 58,OQo 1 PEPPER AVs 165,Ooo WELL DONE WAGYU RIBEYE 195,Ooo MEDIUM WELL 1 ICED LEMON TEA 22,Ooo 1 FUSION TEA LYCHE 28 , OQo 1 NUTTELA BRDWNIES 35 Ooo 503,OQ7 SUBtoTAL 52,815 PB1 25,150 SVC ChrG DUE 580,965'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3f81cba-3ed5-4c7c-84bd-d6daf2d7f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_ocr_text(text):\n",
    "    # Normalize spacing\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix common OCR misreads\n",
    "    text = text.replace('OQo', '000').replace('Ooo', '000').replace(',Ooo', ',000')\n",
    "    text = text.replace(', OQo', ',000').replace(' , ', ' ')\n",
    "    \n",
    "    # Split by items or numbers\n",
    "    items = re.findall(r'(\\d+\\s+[\\w\\s]+?\\s+\\d{2,3},000)', text)\n",
    "    \n",
    "    # Extract total and other charges\n",
    "    subtotal = re.search(r'SUBtoTAL\\s+(\\d{2,3},\\d{3})', text, re.IGNORECASE)\n",
    "    pb1 = re.search(r'PB1\\s+(\\d{2,3},\\d{3})', text)\n",
    "    svc = re.search(r'SVC\\s+ChrG\\s+DUE\\s+(\\d{2,3},\\d{3})', text)\n",
    "\n",
    "    return {\n",
    "        'items': items,\n",
    "        'subtotal': subtotal.group(1) if subtotal else None,\n",
    "        'pb1': pb1.group(1) if pb1 else None,\n",
    "        'total': svc.group(1) if svc else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "862383de-16ea-4440-bd28-a9fc1d776fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from spacy) (1.24.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from spacy) (2.11.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.2.6-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ghiffari\\work-folder\\01a-digital-projects\\2502-01-dbs foundation camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.7-cp311-cp311-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.9 MB 1.5 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.8/14.9 MB 1.6 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.0/14.9 MB 1.2 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.3/14.9 MB 1.2 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.6/14.9 MB 1.3 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 1.8/14.9 MB 1.3 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 2.1/14.9 MB 1.3 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.4/14.9 MB 1.3 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.6/14.9 MB 1.4 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.9/14.9 MB 1.3 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 3.1/14.9 MB 1.3 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 3.4/14.9 MB 1.3 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 3.9/14.9 MB 1.4 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 4.2/14.9 MB 1.4 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.7/14.9 MB 1.4 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 5.2/14.9 MB 1.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.5/14.9 MB 1.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.8/14.9 MB 1.5 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 6.3/14.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 6.6/14.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.8/14.9 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 7.3/14.9 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 7.6/14.9 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 8.1/14.9 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 8.4/14.9 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 8.7/14.9 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 8.9/14.9 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 9.2/14.9 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 9.7/14.9 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.0/14.9 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 10.5/14.9 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.7/14.9 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 11.3/14.9 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 11.5/14.9 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 11.8/14.9 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 12.3/14.9 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 12.6/14.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 13.1/14.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 13.4/14.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.6/14.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.9/14.9 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.4/14.9 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.4/14.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl (117 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 2.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.0/6.2 MB 1.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.3/6.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.2 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.4/6.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 2.9/6.2 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.1/6.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.9/6.2 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.2/6.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.7/6.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading numpy-2.2.6-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.9 MB 1.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.0/12.9 MB 1.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 1.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 1.8 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.1/12.9 MB 1.8 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.6/12.9 MB 1.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.9/12.9 MB 1.8 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.4/12.9 MB 1.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 1.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.9/12.9 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.5/12.9 MB 1.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.7/12.9 MB 1.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.0/12.9 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.5/12.9 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.8/12.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.0/12.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.3/12.9 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.8/12.9 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 7.1/12.9 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.3/12.9 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.9/12.9 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.1/12.9 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.7/12.9 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.9/12.9 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.4/12.9 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.0/12.9 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.2/12.9 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.5/12.9 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.7/12.9 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.0/12.9 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.0/12.9 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.3/12.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.9 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.6/12.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.0/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.9/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.4/5.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, numpy, murmurhash, marisa-trie, cloudpathlib, click, catalogue, srsly, preshed, language-data, blis, typer, langcodes, confection, weasel, thinc, spacy\n",
      "\n",
      "   - --------------------------------------  1/22 [wasabi]\n",
      "   --- ------------------------------------  2/22 [spacy-loggers]\n",
      "   ----- ----------------------------------  3/22 [spacy-legacy]\n",
      "   ----- ----------------------------------  3/22 [spacy-legacy]\n",
      "   ------- --------------------------------  4/22 [smart-open]\n",
      "   ------- --------------------------------  4/22 [smart-open]\n",
      "  Attempting uninstall: numpy\n",
      "   ------- --------------------------------  4/22 [smart-open]\n",
      "    Found existing installation: numpy 1.24.4\n",
      "   ------- --------------------------------  4/22 [smart-open]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "    Uninstalling numpy-1.24.4:\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------- -----------------------------  6/22 [numpy]\n",
      "   ---------------- -----------------------  9/22 [cloudpathlib]\n",
      "   ---------------- -----------------------  9/22 [cloudpathlib]\n",
      "   ---------------- -----------------------  9/22 [cloudpathlib]\n",
      "   ---------------- -----------------------  9/22 [cloudpathlib]\n",
      "   ------------------ --------------------- 10/22 [click]\n",
      "   ------------------ --------------------- 10/22 [click]\n",
      "   -------------------- ------------------- 11/22 [catalogue]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   --------------------- ------------------ 12/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [preshed]\n",
      "   ------------------------- -------------- 14/22 [language-data]\n",
      "   ------------------------- -------------- 14/22 [language-data]\n",
      "   ------------------------- -------------- 14/22 [language-data]\n",
      "   ------------------------- -------------- 14/22 [language-data]\n",
      "   ------------------------- -------------- 14/22 [language-data]\n",
      "   ------------------------- -------------- 14/22 [language-data]\n",
      "   ------------------------- -------------- 14/22 [language-data]\n",
      "   --------------------------- ------------ 15/22 [blis]\n",
      "   --------------------------- ------------ 15/22 [blis]\n",
      "   ----------------------------- ---------- 16/22 [typer]\n",
      "   ----------------------------- ---------- 16/22 [typer]\n",
      "   ----------------------------- ---------- 16/22 [typer]\n",
      "   ------------------------------ --------- 17/22 [langcodes]\n",
      "   ------------------------------ --------- 17/22 [langcodes]\n",
      "   -------------------------------- ------- 18/22 [confection]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   ---------------------------------------- 22/22 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 click-8.2.1 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.13 numpy-2.2.6 preshed-3.0.10 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Ghiffari\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.2.17 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "langchain-community 0.2.17 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "paddlex 3.0.0 requires numpy==1.24.4; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-intel 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0490a706-b488-456a-b756-491a63bc6935",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m----------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlang\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01men\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m English\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EntityRuler\n\u001b[32m      4\u001b[39m nlp = English()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[32m      8\u001b[39m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\spacy\\errors.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\spacy\\compat.py:39\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatalogue\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthinc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     41\u001b[39m pickle = pickle\n\u001b[32m     42\u001b[39m copy_reg = copy_reg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\thinc\\api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     CupyOps,\n\u001b[32m      3\u001b[39m     MPSOps,\n\u001b[32m      4\u001b[39m     NumpyOps,\n\u001b[32m      5\u001b[39m     Ops,\n\u001b[32m      6\u001b[39m     get_current_ops,\n\u001b[32m      7\u001b[39m     get_ops,\n\u001b[32m      8\u001b[39m     set_current_ops,\n\u001b[32m      9\u001b[39m     set_gpu_allocator,\n\u001b[32m     10\u001b[39m     use_ops,\n\u001b[32m     11\u001b[39m     use_pytorch_for_gpu_memory,\n\u001b[32m     12\u001b[39m     use_tensorflow_for_gpu_memory,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\thinc\\backends\\__init__.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_cupy_allocators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_server\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcupy_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmps_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\thinc\\backends\\cupy_ops.py:16\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     is_cupy_array,\n\u001b[32m      8\u001b[39m     is_mxnet_gpu_array,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     torch2xp,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@registry\u001b[39m.ops(\u001b[33m\"\u001b[39m\u001b[33mCupyOps\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCupyOps\u001b[39;00m(Ops):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\thinc\\backends\\numpy_ops.pyx:1\u001b[39m, in \u001b[36minit thinc.backends.numpy_ops\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = English()\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [\n",
    "    {\"label\": \"FOOD_ITEM\", \"pattern\": [{\"IS_DIGIT\": True}, {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"LIKE_NUM\": True}]}\n",
    "]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "doc = nlp(\"1 SPGTHY BOLOGNASE 58,000\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9be643-5078-4493-9612-9df31c984197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_easyocr(folder_path: str = None, label: str = None):\n",
    "    img_path = os.path.join(folder_path, img_name)\n",
    "    results = reader.readtext(img_path)\n",
    "    text_lines = [res[1] for res in results]  # ambil teks dari result\n",
    "\n",
    "    print(json.dumps(parsed, indent=2, ensure_ascii=False))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96ab3d",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be4e6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.services.qcap import process_qcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3a769df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'menu': [{'nm': 'Nasi Campur Bali', 'cnt': '1 x', 'price': '75,000'},\n",
       "  {'nm': 'Bbk Bengil Nasi', 'cnt': '1 x', 'price': '125,000'},\n",
       "  {'nm': 'MilkShake Starwb', 'cnt': '1 x', 'price': '37,000'},\n",
       "  {'nm': 'Ice Lemon Tea', 'cnt': '1 x', 'price': '24,000'},\n",
       "  {'nm': 'Nasi Ayam Dewata', 'cnt': '1 x', 'price': '70,000'},\n",
       "  {'nm': 'Free Ice Tea', 'cnt': '3 x', 'price': '0'},\n",
       "  {'nm': 'Organic Green Sa', 'cnt': '1 x', 'price': '65,000'},\n",
       "  {'nm': 'Ice Tea', 'cnt': '1 x', 'price': '18,000'},\n",
       "  {'nm': 'Ice Orange', 'cnt': '1 x', 'price': '29,000'},\n",
       "  {'nm': 'Ayam Suir Bali', 'cnt': '1 x', 'price': '85,000'},\n",
       "  {'nm': 'Tahu Goreng', 'cnt': '2 x', 'price': '36,000'},\n",
       "  {'nm': 'Tempe Goreng', 'cnt': '2 x', 'price': '36,000'},\n",
       "  {'nm': 'Tahu Telor Asin', 'cnt': '1 x', 'price': '40,000.'},\n",
       "  {'nm': 'Nasi Goreng Samb', 'cnt': '1 x', 'price': '70,000'},\n",
       "  {'nm': 'Bbk Panggang Sam', 'cnt': '3 x', 'price': '366,000'},\n",
       "  {'nm': 'Ayam Sambal Hija', 'cnt': '1 x', 'price': '92,000'},\n",
       "  {'nm': 'Hot Tea', 'cnt': '2 x', 'price': '44,000'},\n",
       "  {'nm': 'Ice Kopi', 'cnt': '1 x', 'price': '32,000'},\n",
       "  {'nm': 'Tahu Telor Asin', 'cnt': '1 x', 'price': '40,000'},\n",
       "  {'nm': 'Free Ice Tea', 'cnt': '1 x', 'price': '0'},\n",
       "  {'nm': 'Bebek Street', 'cnt': '1 x', 'price': '44,000'},\n",
       "  {'nm': 'Ice Tea Tawar', 'cnt': '1 x', 'price': '18,000'}],\n",
       " 'sub_total': {'subtotal_price': '1,346,000',\n",
       "  'service_price': '100,950',\n",
       "  'tax_price': '144,695',\n",
       "  'etc': '-45'},\n",
       " 'total': {'total_price': '1,591,600'}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_qcap(\"inference\\jpgs\\img_00000.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a353a110-67cc-4456-a250-53e5a520d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 524.3/991.5 kB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 786.4/991.5 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 786.4/991.5 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e2ca16-0423-462d-a561-8f9d54a4a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import VisionEncoderDecoderModel, DonutProcessor\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\n",
    "    \"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    \"naver-clova-ix/donut-base-finetuned-cord-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d85f0f5-e323-4937-941b-2983901c63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def process_qcap(path):\n",
    "    image_path = path\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        pixel_values=pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    \n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "    return processor.token2json(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d8a3299-6383-4b38-938a-f3ea6a7a335f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'menu': [{'nm': 'Nasi Campur Bali', 'cnt': '1 x', 'price': '75,000'},\n",
       "  {'nm': 'Bbk Bengil Nasi', 'cnt': '1 x', 'price': '125,000'},\n",
       "  {'nm': 'MilkShake Starwb', 'cnt': '1 x', 'price': '37,000'},\n",
       "  {'nm': 'Ice Lemon Tea', 'cnt': '1 x', 'price': '24,000'},\n",
       "  {'nm': 'Nasi Ayam Dewata', 'cnt': '1 x', 'price': '70,000'},\n",
       "  {'nm': 'Free Ice Tea', 'cnt': '3 x', 'price': '0'},\n",
       "  {'nm': 'Organic Green Sa', 'cnt': '1 x', 'price': '65,000'},\n",
       "  {'nm': 'Ice Tea', 'cnt': '1 x', 'price': '18,000'},\n",
       "  {'nm': 'Ice Orange', 'cnt': '1 x', 'price': '29,000'},\n",
       "  {'nm': 'Ayam Suir Bali', 'cnt': '1 x', 'price': '85,000'},\n",
       "  {'nm': 'Tahu Goreng', 'cnt': '2 x', 'price': '36,000'},\n",
       "  {'nm': 'Tempe Goreng', 'cnt': '2 x', 'price': '36,000'},\n",
       "  {'nm': 'Tahu Telor Asin', 'cnt': '1 x', 'price': '40,000.'},\n",
       "  {'nm': 'Nasi Goreng Samb', 'cnt': '1 x', 'price': '70,000'},\n",
       "  {'nm': 'Bbk Panggang Sam', 'cnt': '3 x', 'price': '366,000'},\n",
       "  {'nm': 'Ayam Sambal Hija', 'cnt': '1 x', 'price': '92,000'},\n",
       "  {'nm': 'Hot Tea', 'cnt': '2 x', 'price': '44,000'},\n",
       "  {'nm': 'Ice Kopi', 'cnt': '1 x', 'price': '32,000'},\n",
       "  {'nm': 'Tahu Telor Asin', 'cnt': '1 x', 'price': '40,000'},\n",
       "  {'nm': 'Free Ice Tea', 'cnt': '1 x', 'price': '0'},\n",
       "  {'nm': 'Bebek Street', 'cnt': '1 x', 'price': '44,000'},\n",
       "  {'nm': 'Ice Tea Tawar', 'cnt': '1 x', 'price': '18,000'}],\n",
       " 'sub_total': {'subtotal_price': '1,346,000',\n",
       "  'service_price': '100,950',\n",
       "  'tax_price': '144,695',\n",
       "  'etc': '-45'},\n",
       " 'total': {'total_price': '1,591,600'}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qcap_out = process_qcap(\"inference\\jpgs\\img_00000.jpg\")\n",
    "qcap_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "540f8e00-03c4-4270-8b37-48e6cbe45265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(qcap_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb0c6c-e32f-4a38-9923-3a09bb0276a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = \"1,591,600\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0e599-970c-4c05-9027-90f0b9df5510",
   "metadata": {},
   "source": [
    "## Qrep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa84f35-8a82-4fea-8368-b098d76c02c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d70bf988-6e45-4dd9-abe3-4725ab0df111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4720ccb-73ee-4aa3-a85d-5f00fecf7bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 7, 4, 1, 2, 0, 8, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([\"lain-lain\",\n",
    "            \"makanan\",\n",
    "            \"minuman\",\n",
    "            \"kesehatan\",\n",
    "            \"elektronik\",\n",
    "            \"fashion\",\n",
    "            \"atk\",\n",
    "            \"transport\",\n",
    "            \"hiburan\"])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c37eec56-38c9-4676-b0e5-411bbee1c408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['atk'], dtype='<U10')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae73da7a-b602-4ff3-a6fc-9f06168a95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TEKS = \"BENSIN\"\n",
    "\n",
    "def predict_category(text):\n",
    "    logits = new_model.predict(tf.constant([text]), verbose=0)\n",
    "    idx = tf.argmax(logits[0]).numpy()\n",
    "    label = le.inverse_transform([idx])[0]\n",
    "    return label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b397a7f-48b8-4857-a737-13eaace6c3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lain-lain'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediksi = predict_category(SAMPLE_TEKS)\n",
    "prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b778d9b-b294-4655-bce0-75f3d55d68f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(le, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d5b14-ed1f-4f6e-97a2-71bd1e29f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "le = joblib.load(\"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41330ed2-8c7b-4b77-bb73-5fcc0ab52f97",
   "metadata": {},
   "source": [
    "# Merged Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2cba726c-c1e7-4ce0-ba22-48a2fc603669",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = process_qrep(qcap_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8f347233-4bfe-4e3b-b651-cb6d0773c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_data(qcap_out: dict):\n",
    "    # Ambil dan ubah menu jadi dataframe\n",
    "    menus = qcap_out.get(\"menu\", [])\n",
    "    data = pd.DataFrame(menus) if isinstance(menus, list) else pd.DataFrame([menus])\n",
    "    data.rename(columns=lambda x: x.strip().lower(), inplace=True)\n",
    "\n",
    "    # Pastikan kolom yang dibutuhkan ada\n",
    "    if 'nm' in data.columns:\n",
    "        data.rename(columns={'nm': 'item'}, inplace=True)\n",
    "    if 'cnt' in data.columns:\n",
    "        data.rename(columns={'cnt': 'qty'}, inplace=True)\n",
    "        data['qty'] = data['qty'].apply(extract_int)\n",
    "    if 'price' in data.columns:\n",
    "        data.rename(columns={'price': 'price'}, inplace=True)\n",
    "        data['price'] = data['price'].apply(str_to_int)\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_qrep(qcap_out: dict,  model=model, le=le, report_period=\"bulanan\"):\n",
    "    \"\"\"\n",
    "    Proses hasil ekstraksi Qcap menjadi format laporan lengkap untuk Qrep.\n",
    "\n",
    "    Args:\n",
    "        qcap_out (dict): Output dari modul Qcap, berisi informasi transaksi.\n",
    "        model: Model klasifikasi kategori item.\n",
    "        le: LabelEncoder untuk label kategori item.\n",
    "        report_period (str): Periode pelaporan, default \"bulanan\".\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON terstruktur berisi detail transaksi dan klasifikasi kategori.\n",
    "    \"\"\"\n",
    "    data = format_data(qcap_out)\n",
    "    # Ambil field utama\n",
    "    merchant_name = qcap_out.get(\"toko\", \"\")\n",
    "    date = qcap_out.get(\"tanggal\", \"\")\n",
    "    total_amount = str_to_int(qcap_out.get(\"total\", 0).get(\"total_price\", 0))\n",
    "    payment_method = qcap_out.get(\"pembayaran\", \"\")\n",
    "\n",
    "    # Klasifikasi kategori\n",
    "    if 'item' in data.columns:\n",
    "        try:\n",
    "            # Tokenisasi dan prediksi\n",
    "            data['kategory'] = data['item'].apply(predict_category)\n",
    "        except Exception as e:\n",
    "            print(f\"Kategori error: {e}\")\n",
    "            data['kategory'] = \"\"\n",
    "    else:\n",
    "        data['kategory'] = \"\"\n",
    "\n",
    "    # Format list item\n",
    "    item_list = data[['item', 'qty', 'price', 'kategory']].to_dict(orient='records')\n",
    "\n",
    "    # Susun output JSON akhir\n",
    "    return {\n",
    "        \"merchant_name\": merchant_name,\n",
    "        \"date\": date,\n",
    "        \"total_amount\": total_amount,\n",
    "        \"item_list\": item_list,\n",
    "        \"payment_method\": payment_method,\n",
    "        \"report_period\": report_period\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7b30d0fc-6b7a-478b-adb6-4c664d3ccb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def str_to_int(s, locale=\"US\"):\n",
    "    \"\"\"\n",
    "    Mengubah string angka format US/ID menjadi integer.\n",
    "    \n",
    "    Args:\n",
    "        s (str): String angka, contoh \"1,591,600.00\" atau \"1.591.600,00\"\n",
    "        locale (str): \"US\" atau \"ID\", default \"US\"\n",
    "    \n",
    "    Returns:\n",
    "        int: Nilai bilangan bulat, desimal akan dibuang\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    \n",
    "    if locale.upper() == \"US\":\n",
    "        # Hapus koma (,) lalu pisahkan dari desimal jika ada\n",
    "        s_clean = re.sub(r\",\", \"\", s).split(\".\")[0]\n",
    "    elif locale.upper() == \"ID\":\n",
    "        # Hapus titik (.) lalu pisahkan dari desimal jika ada\n",
    "        s_clean = re.sub(r\"\\.\", \"\", s).split(\",\")[0]\n",
    "    else:\n",
    "        raise ValueError(\"locale must be either 'US' or 'ID'\")\n",
    "    \n",
    "    return int(s_clean)\n",
    "\n",
    "\n",
    "def extract_int(s):\n",
    "    \"\"\"\n",
    "    Ekstrak digit pertama dari string dan ubah ke int.\n",
    "    \n",
    "    Args:\n",
    "        s (str): String input, contoh \"1 x\", \"  3pcs\", \"4x500ml\"\n",
    "    \n",
    "    Returns:\n",
    "        int: Nilai bilangan bulat pertama yang ditemukan. Jika tidak ada, return 0.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\d+\", s)\n",
    "    return int(match.group()) if match else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8f84d097-9556-4ff4-8b8a-af52327f5c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'merchant_name': '',\n",
       " 'date': '',\n",
       " 'total_amount': 1591600,\n",
       " 'item_list': [{'item': 'Nasi Campur Bali',\n",
       "   'qty': 1,\n",
       "   'price': 75000,\n",
       "   'kategory': 'makanan'},\n",
       "  {'item': 'Bbk Bengil Nasi',\n",
       "   'qty': 1,\n",
       "   'price': 125000,\n",
       "   'kategory': 'makanan'},\n",
       "  {'item': 'MilkShake Starwb',\n",
       "   'qty': 1,\n",
       "   'price': 37000,\n",
       "   'kategory': 'minuman'},\n",
       "  {'item': 'Ice Lemon Tea', 'qty': 1, 'price': 24000, 'kategory': 'minuman'},\n",
       "  {'item': 'Nasi Ayam Dewata',\n",
       "   'qty': 1,\n",
       "   'price': 70000,\n",
       "   'kategory': 'makanan'},\n",
       "  {'item': 'Free Ice Tea', 'qty': 3, 'price': 0, 'kategory': 'minuman'},\n",
       "  {'item': 'Organic Green Sa',\n",
       "   'qty': 1,\n",
       "   'price': 65000,\n",
       "   'kategory': 'lain-lain'},\n",
       "  {'item': 'Ice Tea', 'qty': 1, 'price': 18000, 'kategory': 'minuman'},\n",
       "  {'item': 'Ice Orange', 'qty': 1, 'price': 29000, 'kategory': 'minuman'},\n",
       "  {'item': 'Ayam Suir Bali', 'qty': 1, 'price': 85000, 'kategory': 'makanan'},\n",
       "  {'item': 'Tahu Goreng', 'qty': 2, 'price': 36000, 'kategory': 'makanan'},\n",
       "  {'item': 'Tempe Goreng', 'qty': 2, 'price': 36000, 'kategory': 'makanan'},\n",
       "  {'item': 'Tahu Telor Asin', 'qty': 1, 'price': 40000, 'kategory': 'makanan'},\n",
       "  {'item': 'Nasi Goreng Samb',\n",
       "   'qty': 1,\n",
       "   'price': 70000,\n",
       "   'kategory': 'makanan'},\n",
       "  {'item': 'Bbk Panggang Sam',\n",
       "   'qty': 3,\n",
       "   'price': 366000,\n",
       "   'kategory': 'lain-lain'},\n",
       "  {'item': 'Ayam Sambal Hija',\n",
       "   'qty': 1,\n",
       "   'price': 92000,\n",
       "   'kategory': 'makanan'},\n",
       "  {'item': 'Hot Tea', 'qty': 2, 'price': 44000, 'kategory': 'minuman'},\n",
       "  {'item': 'Ice Kopi', 'qty': 1, 'price': 32000, 'kategory': 'minuman'},\n",
       "  {'item': 'Tahu Telor Asin', 'qty': 1, 'price': 40000, 'kategory': 'makanan'},\n",
       "  {'item': 'Free Ice Tea', 'qty': 1, 'price': 0, 'kategory': 'minuman'},\n",
       "  {'item': 'Bebek Street', 'qty': 1, 'price': 44000, 'kategory': 'makanan'},\n",
       "  {'item': 'Ice Tea Tawar', 'qty': 1, 'price': 18000, 'kategory': 'minuman'}],\n",
       " 'payment_method': '',\n",
       " 'report_period': 'bulanan'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = process_qrep(qcap_out, model=model, le=le)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c65aaac5-0a90-4ba8-ab34-d24b00e115cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_key = qcap_out['menu'].keys()\n",
    "len(res_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb05e189-0387-46ba-a6df-25c257cc13da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nm': {0: 'PKT AYAM'}, 'price': {0: '33,000'}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a101c19c-f94e-4ed3-b5fc-03225ff5558f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39mTraceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m menus = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqcap_out\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmenu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:664\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    658\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    659\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    660\u001b[39m     )\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    663\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    666\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmrecords\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmrecords\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    490\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    491\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    120\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\work-folder\\01a-digital-projects\\2502-01-DBS Foundation Camp\\04-projects\\capstone-project\\qtancy-machine-learning\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:656\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    653\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIf using all scalar values, you must pass an index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m have_series:\n\u001b[32m    659\u001b[39m     index = union_indexes(indexes)\n",
      "\u001b[31mValueError\u001b[39m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "menus = pd.DataFrame(qcap_out['menu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a58afda0-845a-4541-9abe-4f84fd71f67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nm</th>\n",
       "      <th>cnt</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bintang Bremer</td>\n",
       "      <td>1</td>\n",
       "      <td>59,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chicken H-H</td>\n",
       "      <td>1</td>\n",
       "      <td>190,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ades</td>\n",
       "      <td>1</td>\n",
       "      <td>10,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               nm cnt    price\n",
       "0  Bintang Bremer   1   59,000\n",
       "1     Chicken H-H   1  190,000\n",
       "2            Ades   1   10,000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "menus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aed468b2312c231256b5b1b42d64f6f3b027a2b95ff5cf1f77f36d4f84a455a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
